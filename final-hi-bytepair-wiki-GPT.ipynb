{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31b189d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-04T06:19:47.527026Z",
     "iopub.status.busy": "2025-10-04T06:19:47.526463Z",
     "iopub.status.idle": "2025-10-04T12:46:05.392175Z",
     "shell.execute_reply": "2025-10-04T12:46:05.391293Z"
    },
    "papermill": {
     "duration": 23177.870784,
     "end_time": "2025-10-04T12:46:05.393547",
     "exception": false,
     "start_time": "2025-10-04T06:19:47.522763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 45000\n",
      "Reusing existing tokens memmap.\n",
      "Memmap loaded.\n",
      "=== Dataset & Tokenizer stats ===\n",
      "Dataset file: /kaggle/input/dataset-bpt/wikipedia_hindi_500mb.txt\n",
      "Dataset size (bytes): 525,475,460\n",
      "Sample unique words (first 1M chars): 26,741\n",
      "Total tokens: 45,675,759, Unique tokens observed: 44,923\n",
      "Tokenizer vocab size: 45,000\n",
      "Train tokens: 41108183 Val tokens: 4567576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1813767095.py:142: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  data = torch.from_numpy(tokens_np)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 71.86M\n",
      "Device: cuda\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Resumed from iter 52001, best val loss 4.1007\n",
      "step 52500: train 3.5839, val 4.1311, train ppl 36.01, val ppl 62.25\n",
      "Checkpoint saved at step 52500\n",
      "step 53000: train 3.6107, val 4.1150, train ppl 36.99, val ppl 61.25\n",
      "Checkpoint saved at step 53000\n",
      "step 53500: train 3.5749, val 4.1408, train ppl 35.69, val ppl 62.85\n",
      "Checkpoint saved at step 53500\n",
      "step 54000: train 3.5765, val 4.1298, train ppl 35.75, val ppl 62.17\n",
      "Checkpoint saved at step 54000\n",
      "step 54500: train 3.5382, val 4.1328, train ppl 34.41, val ppl 62.35\n",
      "Checkpoint saved at step 54500\n",
      "step 55000: train 3.5502, val 4.1452, train ppl 34.82, val ppl 63.13\n",
      "Checkpoint saved at step 55000\n",
      "step 55500: train 3.5438, val 4.1308, train ppl 34.60, val ppl 62.23\n",
      "Checkpoint saved at step 55500\n",
      "step 56000: train 3.5462, val 4.1246, train ppl 34.68, val ppl 61.84\n",
      "Checkpoint saved at step 56000\n",
      "step 56500: train 3.5428, val 4.1120, train ppl 34.56, val ppl 61.07\n",
      "Checkpoint saved at step 56500\n",
      "step 57000: train 3.5325, val 4.1163, train ppl 34.21, val ppl 61.33\n",
      "Checkpoint saved at step 57000\n",
      "step 57500: train 3.5177, val 4.1004, train ppl 33.71, val ppl 60.36\n",
      "Checkpoint saved at step 57500\n",
      "step 58000: train 3.5721, val 4.1165, train ppl 35.59, val ppl 61.34\n",
      "Checkpoint saved at step 58000\n",
      "step 58500: train 3.5358, val 4.1016, train ppl 34.32, val ppl 60.44\n",
      "Checkpoint saved at step 58500\n",
      "step 59000: train 3.5468, val 4.1236, train ppl 34.70, val ppl 61.78\n",
      "Checkpoint saved at step 59000\n",
      "step 59500: train 3.5267, val 4.1108, train ppl 34.01, val ppl 61.00\n",
      "Checkpoint saved at step 59500\n",
      "step 60000: train 3.5399, val 4.1006, train ppl 34.46, val ppl 60.38\n",
      "Checkpoint saved at step 60000\n",
      "step 60500: train 3.5550, val 4.1174, train ppl 34.99, val ppl 61.40\n",
      "Checkpoint saved at step 60500\n",
      "step 61000: train 3.4928, val 4.1096, train ppl 32.88, val ppl 60.92\n",
      "Checkpoint saved at step 61000\n",
      "step 61500: train 3.4875, val 4.1031, train ppl 32.70, val ppl 60.53\n",
      "Checkpoint saved at step 61500\n",
      "step 62000: train 3.4895, val 4.0991, train ppl 32.77, val ppl 60.28\n",
      "Checkpoint saved at step 62000\n",
      "step 62500: train 3.4657, val 4.0964, train ppl 32.00, val ppl 60.12\n",
      "Checkpoint saved at step 62500\n",
      "step 63000: train 3.5296, val 4.1120, train ppl 34.11, val ppl 61.07\n",
      "Checkpoint saved at step 63000\n",
      "step 63500: train 3.5232, val 4.1225, train ppl 33.89, val ppl 61.72\n",
      "Checkpoint saved at step 63500\n",
      "step 64000: train 3.4546, val 4.0891, train ppl 31.65, val ppl 59.68\n",
      "Checkpoint saved at step 64000\n",
      "step 64500: train 3.5290, val 4.1107, train ppl 34.09, val ppl 60.99\n",
      "Checkpoint saved at step 64500\n",
      "step 65000: train 3.4884, val 4.1023, train ppl 32.73, val ppl 60.48\n",
      "Checkpoint saved at step 65000\n",
      "step 65500: train 3.5430, val 4.1080, train ppl 34.57, val ppl 60.82\n",
      "Checkpoint saved at step 65500\n",
      "step 66000: train 3.5235, val 4.0849, train ppl 33.90, val ppl 59.43\n",
      "Checkpoint saved at step 66000\n",
      "step 66500: train 3.5040, val 4.1094, train ppl 33.25, val ppl 60.91\n",
      "Checkpoint saved at step 66500\n",
      "step 67000: train 3.4654, val 4.1125, train ppl 31.99, val ppl 61.10\n",
      "Checkpoint saved at step 67000\n",
      "step 67500: train 3.4855, val 4.0801, train ppl 32.64, val ppl 59.15\n",
      "Checkpoint saved at step 67500\n",
      "step 68000: train 3.4831, val 4.0937, train ppl 32.56, val ppl 59.96\n",
      "Checkpoint saved at step 68000\n",
      "step 68500: train 3.4658, val 4.1113, train ppl 32.00, val ppl 61.02\n",
      "Checkpoint saved at step 68500\n",
      "step 69000: train 3.4962, val 4.0750, train ppl 32.99, val ppl 58.85\n",
      "Checkpoint saved at step 69000\n",
      "step 69500: train 3.4671, val 4.0897, train ppl 32.04, val ppl 59.72\n",
      "Checkpoint saved at step 69500\n",
      "step 70000: train 3.4747, val 4.1114, train ppl 32.29, val ppl 61.03\n",
      "Checkpoint saved at step 70000\n",
      "step 70500: train 3.4203, val 4.0966, train ppl 30.58, val ppl 60.13\n",
      "Checkpoint saved at step 70500\n",
      "step 71000: train 3.4899, val 4.0911, train ppl 32.78, val ppl 59.81\n",
      "Checkpoint saved at step 71000\n",
      "step 71500: train 3.4462, val 4.0846, train ppl 31.38, val ppl 59.42\n",
      "Checkpoint saved at step 71500\n",
      "step 72000: train 3.4633, val 4.1058, train ppl 31.92, val ppl 60.69\n",
      "Checkpoint saved at step 72000\n",
      "step 72500: train 3.4575, val 4.1014, train ppl 31.74, val ppl 60.42\n",
      "Checkpoint saved at step 72500\n",
      "step 73000: train 3.4560, val 4.0880, train ppl 31.69, val ppl 59.62\n",
      "Checkpoint saved at step 73000\n",
      "step 73500: train 3.4676, val 4.0978, train ppl 32.06, val ppl 60.21\n",
      "Checkpoint saved at step 73500\n",
      "step 74000: train 3.4092, val 4.0954, train ppl 30.24, val ppl 60.06\n",
      "Checkpoint saved at step 74000\n",
      "step 74500: train 3.4578, val 4.1062, train ppl 31.75, val ppl 60.71\n",
      "Checkpoint saved at step 74500\n",
      "step 75000: train 3.4452, val 4.0873, train ppl 31.35, val ppl 59.58\n",
      "Checkpoint saved at step 75000\n",
      "step 75500: train 3.4669, val 4.0958, train ppl 32.04, val ppl 60.09\n",
      "Checkpoint saved at step 75500\n",
      "step 76000: train 3.4404, val 4.1038, train ppl 31.20, val ppl 60.57\n",
      "Checkpoint saved at step 76000\n",
      "step 76500: train 3.4339, val 4.0931, train ppl 31.00, val ppl 59.93\n",
      "Checkpoint saved at step 76500\n",
      "step 77000: train 3.4406, val 4.0916, train ppl 31.21, val ppl 59.84\n",
      "Checkpoint saved at step 77000\n",
      "step 77500: train 3.4210, val 4.0791, train ppl 30.60, val ppl 59.09\n",
      "Checkpoint saved at step 77500\n",
      "step 78000: train 3.3879, val 4.0915, train ppl 29.60, val ppl 59.83\n",
      "Checkpoint saved at step 78000\n",
      "step 78500: train 3.4227, val 4.1103, train ppl 30.65, val ppl 60.97\n",
      "Checkpoint saved at step 78500\n",
      "step 79000: train 3.4368, val 4.0727, train ppl 31.09, val ppl 58.72\n",
      "Checkpoint saved at step 79000\n",
      "step 79500: train 3.4626, val 4.0617, train ppl 31.90, val ppl 58.07\n",
      "Checkpoint saved at step 79500\n",
      "step 79999: train 3.4382, val 4.0870, train ppl 31.13, val ppl 59.56\n",
      "Checkpoint saved at step 79999\n",
      "Generated text:\n",
      " भारत की राजधानी जाफना से ८०० मील की दूरी पर है । इसका नाम ' क्रूस बाज़ार ' ( तमिल : ख द् च . मग म ) ने रखा था । उसके पुरस्कृत प्लस में जेनरल ेय का \" कुशन क्स \" या \" क्रॉ फर्ड \") था , जो हि सुत ल्ला द चतुर्थ ( १० ४१ ई . पू .) की सीट था । राजा चमत्कार ( १० ०४ ई . से ११७ ई . पू .)- इस नगर का नामकरण हिन्दु देवी लाल रस के प्रख्यात यज्ञ से भी किया गया है । इसे सर्वोत्कृष्ट संस्कृत कवि का प्र यु र् वाद पं चानन संभव ने एक ही तत्त्व रूप में रखा है । कुछ लोग इसको \" भ ट् टना काचार्य ज महा प्रज्ञ \" के नाम से जानते हैं । सोम - ये संस्कृत अग्नि को निर्देश में बाध दिया गया है - अग्नि प्रारम्भ के साथ अग्नि प्रथम ।। जो अग्नि विष्णु के प्रमुख देवताओं में से एक हैं , जिसमें सूर्य तथा चन्द्र - इन दोनों तत्त्वों और जीव देवों के भक्तों की शरण स्थली हैं । काशी के भगवान जगन्नाथ भी है । मत कारों की इस नगरी के ंत र्गत को सर्पदंश हूण\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "batch_size = 8\n",
    "block_size = 1024\n",
    "max_iters =  80000         #72000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "\n",
    "checkpoint_last = \"/kaggle/input/53k_itr_gpt_bpt/pytorch/default/1/checkpoint_last.pt\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Load Trained BPE Tokenizer\n",
    "# --------------------------\n",
    "tokenizer = Tokenizer.from_file(\"/kaggle/input/hindi-token/hindi_bpe_tokenizer-45k.json\")\n",
    "\n",
    "def encode(s):\n",
    "    return tokenizer.encode(s).ids\n",
    "\n",
    "def decode(ids):\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "\n",
    "# --------------------------\n",
    "# Memmap Tokenization\n",
    "# --------------------------\n",
    "dataset_path = \"/kaggle/input/dataset-bpt/wikipedia_hindi_500mb.txt\"\n",
    "tokens_memmap_path = \"/kaggle/input/53k_itr_gpt_bpt/pytorch/default/1/tokens.dat\"\n",
    "tokens_len_cache = \"/kaggle/input/53k_itr_gpt_bpt/pytorch/default/1/tokens_len.txt.txt\"\n",
    "chunk_size_chars = 200_000\n",
    "\n",
    "def tokenize_count_pass(path, chunk_size=chunk_size_chars):\n",
    "    total = 0\n",
    "    uniq = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            enc = tokenizer.encode(chunk)\n",
    "            ids = enc.ids\n",
    "            total += len(ids)\n",
    "            uniq.update(ids)\n",
    "    return total, uniq\n",
    "\n",
    "def tokenize_write_memmap(path, total_tokens, memmap_path, chunk_size=chunk_size_chars):\n",
    "    dtype = np.int32\n",
    "    mm = np.memmap(memmap_path, mode=\"w+\", dtype=dtype, shape=(total_tokens,))\n",
    "    pos = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            ids = tokenizer.encode(chunk).ids\n",
    "            L = len(ids)\n",
    "            if L:\n",
    "                mm[pos:pos+L] = np.array(ids, dtype=dtype)\n",
    "                pos += L\n",
    "    if pos != total_tokens:\n",
    "        print(f\"Warning: expected {total_tokens} tokens but wrote {pos}.\")\n",
    "    mm.flush()\n",
    "    return mm\n",
    "\n",
    "# Check for existing memmap\n",
    "use_existing_memmap = True\n",
    "if os.path.exists(tokens_memmap_path) and os.path.exists(tokens_len_cache):\n",
    "    try:\n",
    "        with open(tokens_len_cache, \"r\") as f:\n",
    "            cached_len = int(f.read().strip())\n",
    "        if os.path.getsize(tokens_memmap_path) == cached_len * np.dtype(np.int32).itemsize:\n",
    "            use_existing_memmap = True\n",
    "            total_tokens = cached_len\n",
    "            print(\"Reusing existing tokens memmap.\")\n",
    "    except Exception:\n",
    "        use_existing_memmap = False\n",
    "\n",
    "if not use_existing_memmap:\n",
    "    print(\"Tokenizing (pass 1: counting tokens)...\")\n",
    "    total_tokens, uniq_tokens_set = tokenize_count_pass(dataset_path)\n",
    "    print(f\"Total tokens: {total_tokens:,}, Unique tokens: {len(uniq_tokens_set):,}\")\n",
    "    \n",
    "    print(\"Tokenizing (pass 2: writing memmap)...\")\n",
    "    mm = tokenize_write_memmap(dataset_path, total_tokens, tokens_memmap_path)\n",
    "    with open(tokens_len_cache, \"w\") as f:\n",
    "        f.write(str(total_tokens))\n",
    "    print(\"Memmap written.\")\n",
    "else:\n",
    "    total_tokens = cached_len\n",
    "    mm = np.memmap(tokens_memmap_path, mode=\"r\", dtype=np.int32, shape=(total_tokens,))\n",
    "    uniq_tokens_set = set()\n",
    "    print(\"Memmap loaded.\")\n",
    "\n",
    "# Compute unique tokens safely if not available\n",
    "if len(uniq_tokens_set) == 0:\n",
    "    uniq = set()\n",
    "    step = 2_000_000\n",
    "    for start in range(0, total_tokens, step):\n",
    "        end = min(start + step, total_tokens)\n",
    "        uniq.update(np.unique(mm[start:end]).tolist())\n",
    "    uniq_tokens_set = uniq\n",
    "\n",
    "num_tokens = total_tokens\n",
    "unique_tokens = len(uniq_tokens_set)\n",
    "\n",
    "# Dataset text stats\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_chunk = f.read(1_000_000)\n",
    "num_chars_total_approx = os.path.getsize(dataset_path)\n",
    "unique_words_sample = len(set(first_chunk.split()))\n",
    "\n",
    "print(\"=== Dataset & Tokenizer stats ===\")\n",
    "print(f\"Dataset file: {dataset_path}\")\n",
    "print(f\"Dataset size (bytes): {num_chars_total_approx:,}\")\n",
    "print(f\"Sample unique words (first 1M chars): {unique_words_sample:,}\")\n",
    "print(f\"Total tokens: {num_tokens:,}, Unique tokens observed: {unique_tokens:,}\")\n",
    "print(f\"Tokenizer vocab size: {vocab_size:,}\")\n",
    "\n",
    "# --------------------------\n",
    "# Prepare torch data\n",
    "# --------------------------\n",
    "tokens_np = np.memmap(tokens_memmap_path, mode=\"r\", dtype=np.int32, shape=(total_tokens,))\n",
    "data = torch.from_numpy(tokens_np)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n].long()\n",
    "val_data = data[n:].long()\n",
    "print(\"Train tokens:\", len(train_data), \"Val tokens:\", len(val_data))\n",
    "\n",
    "# --------------------------\n",
    "# Transformer Model\n",
    "# --------------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        return x + self.ffwd(self.ln2(x))\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# --------------------------\n",
    "# Data loader\n",
    "# --------------------------\n",
    "def get_batch(split):\n",
    "    data_tensor = train_data if split==\"train\" else val_data\n",
    "    max_start = len(data_tensor)-block_size-1\n",
    "    if max_start <= 0:\n",
    "        raise ValueError(\"Dataset too small for given block_size\")\n",
    "    ix = torch.randint(0, max_start, (batch_size,))\n",
    "    x = torch.stack([data_tensor[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_tensor[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# --------------------------\n",
    "# Loss & Perplexity estimate\n",
    "# --------------------------\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            _, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        mean_loss = losses.mean()\n",
    "        out[split] = mean_loss\n",
    "        out[split+\"_ppl\"] = math.exp(mean_loss)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# --------------------------\n",
    "# Initialize model, optimizer\n",
    "# --------------------------\n",
    "model = BigramLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Device: {device}\")\n",
    "if device==\"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# --------------------------\n",
    "# Resume checkpoint if exists\n",
    "# --------------------------\n",
    "start_iter = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "history = {\n",
    "    \"step\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_ppl\": [],\n",
    "    \"val_ppl\": []\n",
    "}\n",
    "\n",
    "if os.path.exists(checkpoint_last):\n",
    "    checkpoint = torch.load(checkpoint_last, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    start_iter = checkpoint[\"iter\"] + 1\n",
    "    best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n",
    "    history = checkpoint.get(\"history\", history)\n",
    "    print(f\"Resumed from iter {start_iter}, best val loss {best_val_loss:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Training loop\n",
    "# --------------------------\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "last_ckpt = os.path.join(checkpoint_dir, \"checkpoint_last-af-52.pt\")\n",
    "\n",
    "for iter in range(start_iter, max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters-1:\n",
    "        stats = estimate_loss()\n",
    "        val_loss = stats[\"val\"]\n",
    "        print(f\"step {iter}: train {stats['train']:.4f}, val {val_loss:.4f}, \"\n",
    "              f\"train ppl {stats['train_ppl']:.2f}, val ppl {stats['val_ppl']:.2f}\")\n",
    "\n",
    "        # Store history\n",
    "        history[\"step\"].append(iter)\n",
    "        history[\"train_loss\"].append(stats[\"train\"])\n",
    "        history[\"val_loss\"].append(stats[\"val\"])\n",
    "        history[\"train_ppl\"].append(stats[\"train_ppl\"])\n",
    "        history[\"val_ppl\"].append(stats[\"val_ppl\"])\n",
    "\n",
    "        # Save last checkpoint\n",
    "        torch.save({\n",
    "            \"iter\": iter,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"history\": history\n",
    "        }, last_ckpt)\n",
    "        print(f\"Checkpoint saved at step {iter}\")\n",
    "\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# --------------------------\n",
    "# Generate sample text\n",
    "# --------------------------\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "# out = model.generate(context, max_new_tokens=200)\n",
    "# print(\"Generated text:\\n\", decode(out[0].tolist()))\n",
    "prompt = \"भारत की राजधानी\"\n",
    "context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "out = model.generate(context, max_new_tokens=200)\n",
    "print(\"Generated text:\\n\", decode(out[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594e7ba",
   "metadata": {
    "papermill": {
     "duration": 0.005781,
     "end_time": "2025-10-04T12:46:05.405811",
     "exception": false,
     "start_time": "2025-10-04T12:46:05.400030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8396479,
     "sourceId": 13250897,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8396506,
     "sourceId": 13250931,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 463784,
     "modelInstanceId": 447339,
     "sourceId": 597387,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23185.186111,
   "end_time": "2025-10-04T12:46:08.200653",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-04T06:19:43.014542",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
