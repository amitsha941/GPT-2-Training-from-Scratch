# GPT-2-Training-from-Scratch
Built and trained a GPT-2â€“style transformer from scratch using PyTorch on a Hindi Wikipedia dataset with a custom 45K BPE tokenizer.
