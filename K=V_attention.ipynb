{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13779643,"sourceType":"datasetVersion","datasetId":8770900},{"sourceId":13837412,"sourceType":"datasetVersion","datasetId":8812861},{"sourceId":13954304,"sourceType":"datasetVersion","datasetId":8894477},{"sourceId":13955874,"sourceType":"datasetVersion","datasetId":8895614},{"sourceId":13956196,"sourceType":"datasetVersion","datasetId":8895843}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# train_gpt_memmap_with_perplexity.py\n# Full script with: memmap-safe dataset builder + GPT training (RMSNorm + SwiGLU + Attention V=K)\n# Adds: validation perplexity calculation + saving of checkpoints, config.json, and metrics.jsonl\n\nimport os\nimport json\nimport time\nimport math\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------\n# Config - edit paths & hyperparams\n# -----------------------\nDATA_DIR = \"/kaggle/input/jklu-en-memap-5gb\"   # where wikipedia_tokens.bin & wikipedia_tokens_meta.json live\nSINGLEFILE_DIR = \"/kaggle/input/jklu-en5gb-singlefile\"  # fallback dir for merged_wikipedia.txt\nBIN_NAME = \"wikipedia_tokens.bin\"\nMETA_NAME = \"wikipedia_tokens_meta.json\"\nSINGLEFILE_NAME = \"merged_wikipedia.txt\"\n\n# Data selection: use only first TRAIN_PERCENT of memmap tokens\nTRAIN_PERCENT = 0.70        # <-- use 70% of full memmap tokens\nTRAIN_VAL_SPLIT = 0.70      # <-- within the selected tokens, train=70%, val=30%\n\n# Low-memory safe defaults (change if you know your GPU can handle more)\nSEQ_LEN = 256        # context length (reduce to save memory)\nBATCH_SIZE = 2       # per-step batch size (increase if you have more memory)\nGRAD_ACCUM = 1       # accumulate gradients to emulate bigger batch\nEPOCHS = 1\nLR = 2e-4\nNUM_WORKERS = 0      # set to 0 for low-memory, increase if you have spare CPU RAM\nPIN_MEMORY = False\nSAVE_EVERY_STEPS = 2000\nOUTPUT_DIR = \"./out_ckpt\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nMETRICS_LOG = os.path.join(OUTPUT_DIR, \"metrics.jsonl\")\nCONFIG_PATH = os.path.join(OUTPUT_DIR, \"config.json\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Chunking when building memmap from text\nTOKENIZE_CHUNK_LINES = 1000   # number of lines per chunk to pass to tokenizer\n\n# -----------------------\n# Utils: meta & memmap\n# -----------------------\ndef load_meta(meta_path: str) -> dict:\n    if not os.path.exists(meta_path):\n        return {}\n    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef infer_dtype_from_meta(meta: dict) -> np.dtype:\n    if 'dtype' in meta:\n        return np.dtype(meta['dtype'])\n    if 'numpy_dtype' in meta:\n        return np.dtype(meta['numpy_dtype'])\n    return np.int32\n\ndef load_memmap_tokens(bin_path: str, meta_path: Optional[str] = None) -> np.memmap:\n    meta = load_meta(meta_path) if meta_path and os.path.exists(meta_path) else {}\n    dtype = infer_dtype_from_meta(meta)\n    if 'num_tokens' in meta:\n        length = int(meta['num_tokens'])\n    elif 'length' in meta:\n        length = int(meta['length'])\n    else:\n        fsize = os.path.getsize(bin_path)\n        length = fsize // dtype.itemsize\n    print(f\"Opening memmap: {bin_path} (dtype={dtype}, length={length:,})\")\n    mem = np.memmap(bin_path, dtype=dtype, mode='r', shape=(length,))\n    return mem\n\n# -----------------------\n# Build memmap from text (chunked, safe)\n# -----------------------\ndef build_memmap_from_text_streaming(text_path: str, out_bin: str, out_meta: str,\n                                     tokenizer_name: Optional[str] = None):\n    print(\"ðŸ”¨ Building memmap from text (streaming chunked tokenizer)...\")\n    use_hf = False\n    tok = None\n    if tokenizer_name:\n        try:\n            from transformers import AutoTokenizer\n            tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n            use_hf = True\n            print(f\"Using HF tokenizer: {tokenizer_name}\")\n        except Exception:\n            print(\"Could not load HF tokenizer â€” falling back to byte-level chunking.\")\n            use_hf = False\n\n    if os.path.exists(out_bin):\n        print(\"Removing existing incomplete bin file:\", out_bin)\n        os.remove(out_bin)\n\n    total_tokens = 0\n    dtype = np.int32\n\n    with open(text_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        chunk_lines = []\n        for lineno, line in enumerate(f, 1):\n            chunk_lines.append(line)\n            if lineno % TOKENIZE_CHUNK_LINES == 0:\n                txt = \"\".join(chunk_lines)\n                if use_hf:\n                    enc = tok(txt, add_special_tokens=False, return_attention_mask=False)\n                    ids = np.array(enc[\"input_ids\"], dtype=np.int32)\n                else:\n                    b = txt.encode(\"utf-8\", errors=\"ignore\")\n                    ids = np.frombuffer(b, dtype=np.uint8).astype(np.int32)\n                if ids.size:\n                    with open(out_bin, \"ab\") as wf:\n                        wf.write(ids.tobytes())\n                    total_tokens += ids.size\n                chunk_lines = []\n                print(f\"  tokenized lines up to {lineno:,}  -> tokens total {total_tokens:,}\")\n        # final small chunk\n        if chunk_lines:\n            txt = \"\".join(chunk_lines)\n            if use_hf:\n                enc = tok(txt, add_special_tokens=False, return_attention_mask=False)\n                ids = np.array(enc[\"input_ids\"], dtype=np.int32)\n            else:\n                b = txt.encode(\"utf-8\", errors=\"ignore\")\n                ids = np.frombuffer(b, dtype=np.uint8).astype(np.int32)\n            if ids.size:\n                with open(out_bin, \"ab\") as wf:\n                    wf.write(ids.tobytes())\n                total_tokens += ids.size\n            print(f\"  final chunk tokenized -> tokens total {total_tokens:,}\")\n\n    meta = {\n        \"dtype\": str(dtype),\n        \"num_tokens\": int(total_tokens),\n        \"tokenizer\": tokenizer_name if tokenizer_name else \"byte_level_fallback\"\n    }\n    with open(out_meta, \"w\", encoding=\"utf-8\") as f:\n        json.dump(meta, f, indent=2)\n    print(f\"âœ… Wrote memmap bin {out_bin} ({total_tokens:,} tokens) and meta {out_meta}.\")\n\n# -----------------------\n# Dataset: sample context windows from memmap\n# -----------------------\nclass MemmapTokenDataset(Dataset):\n    def __init__(self, memmap: np.memmap, seq_len: int, split: str = \"train\", split_ratio=0.98):\n        assert split in (\"train\", \"val\")\n        N = memmap.shape[0]\n        split_idx = int(N * split_ratio)\n        if split == \"train\":\n            self.start = 0\n            self.end = split_idx\n        else:\n            self.start = split_idx\n            self.end = N\n        self.seq_len = seq_len\n        self.memmap = memmap\n        self.num_examples = max(0, (self.end - self.start) - seq_len)\n    def __len__(self):\n        return self.num_examples\n    def __getitem__(self, idx):\n        pos = self.start + idx\n        seq = np.array(self.memmap[pos: pos + self.seq_len + 1], dtype=np.int64)  # +1 for target shift\n        input_ids = torch.from_numpy(seq[:-1]).long()\n        target_ids = torch.from_numpy(seq[1:]).long()\n        return input_ids, target_ids\n\ndef collate_batch(batch):\n    inputs = torch.stack([b[0] for b in batch], dim=0)\n    targets = torch.stack([b[1] for b in batch], dim=0)\n    return inputs, targets\n\n# -----------------------\n# Model components: RMSNorm, SwiGLU, Attention (V=K)\n# -----------------------\nclass RMSNormSimple(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        rms = x.pow(2).mean(-1, keepdim=True).add(self.eps).sqrt()\n        return x / rms * self.weight\n\nclass SwiGLUFFN_simple(nn.Module):\n    def __init__(self, d_model: int, expansion: int = 4, dropout: float = 0.1):\n        super().__init__()\n        hidden = d_model * expansion\n        self.project_in = nn.Linear(d_model, hidden * 2)\n        self.project_out = nn.Linear(hidden, d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        x_proj = self.project_in(x)\n        a, b = x_proj.chunk(2, dim=-1)\n        x_ff = a * torch.nn.functional.silu(b)\n        x_out = self.project_out(x_ff)\n        return self.dropout(x_out)\n\nclass CausalKVAttention_simple(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n        self.scale = (self.head_dim) ** -0.5\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n        self.attn_dropout = nn.Dropout(dropout)\n        self.proj_dropout = nn.Dropout(dropout)\n    def _shape(self, x):\n        B, T, D = x.shape\n        return x.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n    def _unshape(self, x):\n        B, nh, T, hd = x.shape\n        return x.transpose(1, 2).contiguous().view(B, T, nh * hd)\n    def forward(self, x, cache_k: Optional[torch.Tensor] = None):\n        B, T, D = x.shape\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        qh = self._shape(q)\n        kh = self._shape(k)\n        vh = kh  # V = K\n        if cache_k is not None:\n            kh = torch.cat([cache_k, kh], dim=2)\n            vh = kh\n        T_total = kh.size(2)\n        scores = torch.matmul(qh, kh.transpose(-2, -1)) * self.scale\n        device = x.device\n        causal_mask = torch.triu(torch.ones((T, T_total), dtype=torch.bool, device=device), diagonal=1)\n        scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n        attn = torch.nn.functional.softmax(scores, dim=-1)\n        attn = self.attn_dropout(attn)\n        ctx = torch.matmul(attn, vh)\n        ctx = self._unshape(ctx)\n        out = self.out_proj(ctx)\n        out = self.proj_dropout(out)\n        new_cache_k = kh if cache_k is not None else None\n        return out, new_cache_k\n\nclass GPTBlockKVasK_simple(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, ffn_expansion: int = 4,\n                 attn_dropout: float = 0.1, ffn_dropout: float = 0.1):\n        super().__init__()\n        self.rms1 = RMSNormSimple(d_model)\n        self.attn = CausalKVAttention_simple(d_model, n_heads, dropout=attn_dropout)\n        self.rdrop1 = nn.Dropout(attn_dropout)\n        self.rms2 = RMSNormSimple(d_model)\n        self.ffn = SwiGLUFFN_simple(d_model, expansion=ffn_expansion, dropout=ffn_dropout)\n        self.rdrop2 = nn.Dropout(ffn_dropout)\n    def forward(self, x, cache_k: Optional[torch.Tensor] = None):\n        x_norm = self.rms1(x)\n        attn_out, new_cache_k = self.attn(x_norm, cache_k=cache_k)\n        x = x + self.rdrop1(attn_out)\n        x_norm = self.rms2(x)\n        ffn_out = self.ffn(x_norm)\n        x = x + self.rdrop2(ffn_out)\n        return x, new_cache_k\n\nclass MiniGPT(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int = 512, n_heads: int = 8, n_layers: int = 6,\n                 seq_len: int = 256, ffn_expansion: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n        self.blocks = nn.ModuleList([GPTBlockKVasK_simple(d_model, n_heads, ffn_expansion, dropout, dropout)\n                                     for _ in range(n_layers)])\n        self.ln_f = RMSNormSimple(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n        self.seq_len = seq_len\n    def forward(self, input_ids, cache_k: Optional[torch.Tensor] = None):\n        B, T = input_ids.shape\n        x = self.tok_emb(input_ids)\n        x = x + self.pos_emb[:, :T, :]\n        new_cache = None\n        for blk in self.blocks:\n            x, new_cache = blk(x, cache_k=cache_k)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        return logits, new_cache\n\n# -----------------------\n# Helpers: save checkpoint & append metrics\n# -----------------------\ndef save_checkpoint(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, global_step: int, extra: dict):\n    payload = {\n        \"model_state\": model.state_dict(),\n        \"optimizer_state\": optimizer.state_dict(),\n        \"global_step\": global_step,\n        \"extra\": extra\n    }\n    torch.save(payload, path)\n    # also save config.json with extra (overwrites but that's fine)\n    config = extra.get(\"config\", {})\n    with open(CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=2)\n\ndef append_metrics(epoch: int, global_step: int, val_loss: float, perplexity: float):\n    rec = {\n        \"timestamp\": time.time(),\n        \"epoch\": epoch,\n        \"global_step\": global_step,\n        \"val_loss\": float(val_loss),\n        \"perplexity\": float(perplexity)\n    }\n    with open(METRICS_LOG, \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(rec) + \"\\n\")\n\n# -----------------------\n# Training function: memmap prints + safe loop with AMP & OOM handling\n# -----------------------\ndef train():\n    print(\"\\n==============================\")\n    print(\"ðŸ“‚ Checking dataset & memmap files\")\n    print(\"==============================\\n\")\n\n    bin_path = os.path.join(DATA_DIR, BIN_NAME)\n    meta_path = os.path.join(DATA_DIR, META_NAME)\n    text_path = os.path.join(SINGLEFILE_DIR, SINGLEFILE_NAME)\n\n    print(f\"Looking for bin: {bin_path}\")\n    print(f\"Looking for meta: {meta_path}\")\n\n    if not os.path.exists(bin_path):\n        print(\"âŒ Memmap .bin not found.\")\n        if os.path.exists(text_path):\n            print(f\"Found fallback text: {text_path}\")\n            build_memmap_from_text_streaming(text_path, bin_path, meta_path, tokenizer_name=None)\n        else:\n            raise FileNotFoundError(f\"Neither {bin_path} nor {text_path} found. Provide dataset.\")\n\n    # load memmap\n    mem = load_memmap_tokens(bin_path, meta_path)\n    total_tokens = len(mem)\n    print(f\"âœ”ï¸ Loaded memmap: shape={mem.shape}, dtype={mem.dtype}, total tokens={total_tokens:,}\")\n\n    # -------------------------------\n    # Limit dataset to TRAIN_PERCENT\n    # -------------------------------\n    MAX_TOKENS = int(total_tokens * TRAIN_PERCENT)\n    print(f\"ðŸ“ Using only first {TRAIN_PERCENT*100:.0f}% of data â†’ {MAX_TOKENS:,} tokens\")\n    mem = mem[:MAX_TOKENS]\n\n    meta = load_meta(meta_path) if os.path.exists(meta_path) else {}\n    vocab_size = meta.get(\"vocab_size\", None)\n    if vocab_size is None:\n        try:\n            vocab_size = int(np.max(mem)) + 1\n            print(f\"Inferred vocab_size: {vocab_size}\")\n        except Exception:\n            vocab_size = 65536\n            print(f\"Could not infer vocab_size. Defaulting to {vocab_size}\")\n\n    print(\"\\nðŸ“¦ Creating datasets (on sliced memmap)...\")\n    train_ds = MemmapTokenDataset(mem, seq_len=SEQ_LEN, split=\"train\", split_ratio=TRAIN_VAL_SPLIT)\n    val_ds = MemmapTokenDataset(mem, seq_len=SEQ_LEN, split=\"val\", split_ratio=TRAIN_VAL_SPLIT)\n    print(f\"âœ”ï¸ Train samples: {len(train_ds):,}\")\n    print(f\"âœ”ï¸ Val samples  : {len(val_ds):,}\")\n\n    print(\"\\nðŸ“¦ Building DataLoaders...\")\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                              collate_fn=collate_batch, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n                            collate_fn=collate_batch, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n    print(\"DataLoaders ready.\")\n\n    print(\"\\nðŸ§  Initializing model...\")\n    model = MiniGPT(vocab_size=vocab_size, d_model=512, n_heads=8, n_layers=6, seq_len=SEQ_LEN).to(DEVICE)\n    print(\"Model on device:\", DEVICE)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n\n    # write config file (initial)\n    config = {\n        \"DATA_DIR\": DATA_DIR,\n        \"BIN_NAME\": BIN_NAME,\n        \"META_NAME\": META_NAME,\n        \"TRAIN_PERCENT\": TRAIN_PERCENT,\n        \"TRAIN_VAL_SPLIT\": TRAIN_VAL_SPLIT,\n        \"SEQ_LEN\": SEQ_LEN,\n        \"BATCH_SIZE\": BATCH_SIZE,\n        \"GRAD_ACCUM\": GRAD_ACCUM,\n        \"EPOCHS\": EPOCHS,\n        \"LR\": LR,\n        \"NUM_WORKERS\": NUM_WORKERS,\n        \"DEVICE\": DEVICE,\n        \"vocab_size\": int(vocab_size)\n    }\n    with open(CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=2)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n    global_step = 0\n    model.train()\n\n    for epoch in range(EPOCHS):\n        print(\"\\n=====================================\")\n        print(f\"ðŸš€ Starting Epoch {epoch+1}/{EPOCHS}\")\n        print(\"=====================================\\n\")\n        epoch_start = time.time()\n        steps_per_epoch = len(train_loader)\n        print(f\"Steps this epoch: {steps_per_epoch}, BATCH_SIZE: {BATCH_SIZE}, SEQ_LEN: {SEQ_LEN}\")\n\n        for batch_idx, (input_ids, target_ids) in enumerate(train_loader):\n            # move to device\n            input_ids = input_ids.to(DEVICE, non_blocking=True)\n            target_ids = target_ids.to(DEVICE, non_blocking=True)\n\n            # forward / backward with AMP\n            try:\n                with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n                    logits, _ = model(input_ids)\n                    loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n                    loss = loss / GRAD_ACCUM\n\n                scaler.scale(loss).backward()\n\n                if (batch_idx + 1) % GRAD_ACCUM == 0:\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                    global_step += 1\n\n                # periodic prints\n                if global_step % 50 == 0:\n                    print(f\"[Epoch {epoch+1}] Global step {global_step} | batch {batch_idx+1}/{steps_per_epoch} | loss {loss.item()*GRAD_ACCUM:.6f}\")\n\n            except RuntimeError as e:\n                if 'out of memory' in str(e).lower():\n                    print(f\"âš ï¸ OOM at batch {batch_idx+1} â€” clearing grad/cache and skipping step.\")\n                    optimizer.zero_grad()\n                    if DEVICE == \"cuda\":\n                        torch.cuda.empty_cache()\n                    continue\n                else:\n                    raise\n\n            # ETA every 200 batch reads\n            if (batch_idx + 1) % 200 == 0:\n                elapsed = time.time() - epoch_start\n                steps_done = batch_idx + 1\n                avg_step = elapsed / max(1, steps_done)\n                eta = (steps_per_epoch - steps_done) * avg_step\n                print(f\"â³ ETA for epoch: {eta/60:.2f} minutes\")\n\n            # checkpointing\n            if global_step and global_step % SAVE_EVERY_STEPS == 0:\n                ckpt_path = os.path.join(OUTPUT_DIR, f\"ckpt_step{global_step}.pt\")\n                save_checkpoint(ckpt_path, model, optimizer, global_step, {\"config\": config})\n                print(\"ðŸ’¾ Saved checkpoint ->\", ckpt_path)\n                if DEVICE == \"cuda\":\n                    torch.cuda.empty_cache()\n\n        epoch_time = time.time() - epoch_start\n        print(f\"\\nðŸ Epoch {epoch+1} finished in {epoch_time/60:.2f} minutes\")\n\n        # validation (small sample) â€” compute average loss and perplexity\n        print(\"\\nðŸ” Running quick validation sample...\")\n        model.eval()\n        with torch.no_grad():\n            val_losses = []\n            for i, (input_ids, target_ids) in enumerate(val_loader):\n                input_ids = input_ids.to(DEVICE, non_blocking=True)\n                target_ids = target_ids.to(DEVICE, non_blocking=True)\n                logits, _ = model(input_ids)\n                loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n                val_losses.append(loss.item())\n                if i >= 50:  # sample up to 51 batches for a reasonable estimate (adjustable)\n                    break\n            if val_losses:\n                avg_val_loss = float(sum(val_losses) / len(val_losses))\n                # perplexity = exp(avg_val_loss)\n                try:\n                    perp = float(math.exp(avg_val_loss))\n                    # protect against overflow\n                    if math.isinf(perp) or perp > 1e30:\n                        perp = float('inf')\n                except OverflowError:\n                    perp = float('inf')\n                print(f\"âœ”ï¸ Validation loss (sample of {len(val_losses)} batches): {avg_val_loss:.5f}\")\n                print(f\"âœ”ï¸ Perplexity (sample): {perp:.3f}\")\n                # append metrics to log file\n                append_metrics(epoch + 1, global_step, avg_val_loss, perp)\n                # save checkpoint for this epoch (with metrics)\n                ckpt_path = os.path.join(OUTPUT_DIR, f\"ckpt_epoch{epoch+1}_step{global_step}.pt\")\n                extra = {\"config\": config, \"val_loss\": avg_val_loss, \"perplexity\": perp, \"epoch\": epoch + 1}\n                save_checkpoint(ckpt_path, model, optimizer, global_step, extra)\n                print(\"ðŸ’¾ Saved epoch checkpoint ->\", ckpt_path)\n        model.train()\n\n    # final save + config and metrics\n    final_path = os.path.join(OUTPUT_DIR, \"final_ckpt.pt\")\n    save_checkpoint(final_path, model, optimizer, global_step, {\"config\": config})\n    print(\"Training complete. Saved final checkpoint ->\", final_path)\n    print(\"Config saved to:\", CONFIG_PATH)\n    print(\"Metrics log appended to:\", METRICS_LOG)\n\nif __name__ == \"__main__\":\n    train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:33:49.771390Z","iopub.execute_input":"2025-12-01T17:33:49.771830Z","iopub.status.idle":"2025-12-01T17:35:34.082567Z","shell.execute_reply.started":"2025-12-01T17:33:49.771787Z","shell.execute_reply":"2025-12-01T17:35:34.081533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_gpt_inference.py\n# Load trained checkpoint and generate text from prompts\n\nimport os\nimport json\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom typing import Optional\n\n# -----------------------\n# Config - edit paths\n# -----------------------\nCHECKPOINT_PATH = \"/kaggle/input/46k-itr/ckpt_step46000.pt\"  # or any specific checkpoint\nCONFIG_PATH = \"/kaggle/input/confic/confic.json\"\nMETA_PATH = \"/kaggle/input/jklu-en-memap-5gb/wikipedia_tokens_meta.json\"  # for vocab info\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMAX_NEW_TOKENS = 100  # how many tokens to generate\nTEMPERATURE = 0.8     # higher = more random, lower = more deterministic\nTOP_K = 50            # only sample from top-k tokens (0 = disabled)\n\n# -----------------------\n# Model components (same as training)\n# -----------------------\nclass RMSNormSimple(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        rms = x.pow(2).mean(-1, keepdim=True).add(self.eps).sqrt()\n        return x / rms * self.weight\n\nclass SwiGLUFFN_simple(nn.Module):\n    def __init__(self, d_model: int, expansion: int = 4, dropout: float = 0.1):\n        super().__init__()\n        hidden = d_model * expansion\n        self.project_in = nn.Linear(d_model, hidden * 2)\n        self.project_out = nn.Linear(hidden, d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        x_proj = self.project_in(x)\n        a, b = x_proj.chunk(2, dim=-1)\n        x_ff = a * torch.nn.functional.silu(b)\n        x_out = self.project_out(x_ff)\n        return self.dropout(x_out)\n\nclass CausalKVAttention_simple(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n        self.scale = (self.head_dim) ** -0.5\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n        self.attn_dropout = nn.Dropout(dropout)\n        self.proj_dropout = nn.Dropout(dropout)\n    def _shape(self, x):\n        B, T, D = x.shape\n        return x.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n    def _unshape(self, x):\n        B, nh, T, hd = x.shape\n        return x.transpose(1, 2).contiguous().view(B, T, nh * hd)\n    def forward(self, x, cache_k: Optional[torch.Tensor] = None):\n        B, T, D = x.shape\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        qh = self._shape(q)\n        kh = self._shape(k)\n        vh = kh  # V = K\n        if cache_k is not None:\n            kh = torch.cat([cache_k, kh], dim=2)\n            vh = kh\n        T_total = kh.size(2)\n        scores = torch.matmul(qh, kh.transpose(-2, -1)) * self.scale\n        device = x.device\n        causal_mask = torch.triu(torch.ones((T, T_total), dtype=torch.bool, device=device), diagonal=1)\n        scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n        attn = torch.nn.functional.softmax(scores, dim=-1)\n        attn = self.attn_dropout(attn)\n        ctx = torch.matmul(attn, vh)\n        ctx = self._unshape(ctx)\n        out = self.out_proj(ctx)\n        out = self.proj_dropout(out)\n        new_cache_k = kh if cache_k is not None else None\n        return out, new_cache_k\n\nclass GPTBlockKVasK_simple(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, ffn_expansion: int = 4,\n                 attn_dropout: float = 0.1, ffn_dropout: float = 0.1):\n        super().__init__()\n        self.rms1 = RMSNormSimple(d_model)\n        self.attn = CausalKVAttention_simple(d_model, n_heads, dropout=attn_dropout)\n        self.rdrop1 = nn.Dropout(attn_dropout)\n        self.rms2 = RMSNormSimple(d_model)\n        self.ffn = SwiGLUFFN_simple(d_model, expansion=ffn_expansion, dropout=ffn_dropout)\n        self.rdrop2 = nn.Dropout(ffn_dropout)\n    def forward(self, x, cache_k: Optional[torch.Tensor] = None):\n        x_norm = self.rms1(x)\n        attn_out, new_cache_k = self.attn(x_norm, cache_k=cache_k)\n        x = x + self.rdrop1(attn_out)\n        x_norm = self.rms2(x)\n        ffn_out = self.ffn(x_norm)\n        x = x + self.rdrop2(ffn_out)\n        return x, new_cache_k\n\nclass MiniGPT(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int = 512, n_heads: int = 8, n_layers: int = 6,\n                 seq_len: int = 256, ffn_expansion: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n        self.blocks = nn.ModuleList([GPTBlockKVasK_simple(d_model, n_heads, ffn_expansion, dropout, dropout)\n                                     for _ in range(n_layers)])\n        self.ln_f = RMSNormSimple(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n        self.seq_len = seq_len\n    def forward(self, input_ids, cache_k: Optional[torch.Tensor] = None):\n        B, T = input_ids.shape\n        x = self.tok_emb(input_ids)\n        x = x + self.pos_emb[:, :T, :]\n        new_cache = None\n        for blk in self.blocks:\n            x, new_cache = blk(x, cache_k=cache_k)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        return logits, new_cache\n\n# -----------------------\n# Load checkpoint\n# -----------------------\ndef load_checkpoint(ckpt_path: str, device: str = \"cpu\"):\n    print(f\"Loading checkpoint from: {ckpt_path}\")\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    return checkpoint\n\n# -----------------------\n# Tokenizer support: tiktoken, HuggingFace, or byte-level\n# -----------------------\n_tokenizer_cache = None\n\ndef get_tokenizer(tokenizer_type: str):\n    \"\"\"Get or create tokenizer (cached)\"\"\"\n    global _tokenizer_cache\n    if _tokenizer_cache is not None:\n        return _tokenizer_cache\n    \n    if tokenizer_type == \"byte_level\":\n        _tokenizer_cache = None\n        return None\n    \n    # Try tiktoken first (for GPT-2, GPT-3 style tokenizers)\n    if \"tiktoken\" in tokenizer_type.lower() or \"gpt\" in tokenizer_type.lower():\n        try:\n            import tiktoken\n            # tiktoken_gpt2 maps to gpt2 encoding\n            encoding_name = \"gpt2\"  # or \"cl100k_base\" for GPT-3.5/4\n            enc = tiktoken.get_encoding(encoding_name)\n            _tokenizer_cache = (\"tiktoken\", enc)\n            print(f\"âœ… Loaded tiktoken encoder: {encoding_name}\")\n            return _tokenizer_cache\n        except ImportError:\n            print(\"âš ï¸ tiktoken not installed. Install with: pip install tiktoken\")\n        except Exception as e:\n            print(f\"âš ï¸ Could not load tiktoken: {e}\")\n    \n    # Try HuggingFace tokenizer\n    try:\n        from transformers import AutoTokenizer\n        tok = AutoTokenizer.from_pretrained(tokenizer_type, use_fast=True)\n        _tokenizer_cache = (\"hf\", tok)\n        print(f\"âœ… Loaded HF tokenizer: {tokenizer_type}\")\n        return _tokenizer_cache\n    except Exception as e:\n        print(f\"âš ï¸ Could not load HF tokenizer: {e}\")\n    \n    _tokenizer_cache = None\n    return None\n\ndef encode_text(text: str, tokenizer_type: str = \"byte_level\") -> list:\n    \"\"\"Convert text to token IDs\"\"\"\n    if tokenizer_type == \"byte_level\":\n        return list(text.encode(\"utf-8\", errors=\"ignore\"))\n    \n    tok = get_tokenizer(tokenizer_type)\n    if tok is None:\n        print(\"âš ï¸ Falling back to byte-level encoding\")\n        return list(text.encode(\"utf-8\", errors=\"ignore\"))\n    \n    tok_type, tok_obj = tok\n    if tok_type == \"tiktoken\":\n        return tok_obj.encode(text)\n    elif tok_type == \"hf\":\n        return tok_obj.encode(text, add_special_tokens=False)\n    else:\n        return list(text.encode(\"utf-8\", errors=\"ignore\"))\n\ndef decode_tokens(tokens: list, tokenizer_type: str = \"byte_level\") -> str:\n    \"\"\"Convert token IDs back to text\"\"\"\n    if tokenizer_type == \"byte_level\":\n        try:\n            return bytes(tokens).decode(\"utf-8\", errors=\"ignore\")\n        except:\n            return \"\"\n    \n    tok = get_tokenizer(tokenizer_type)\n    if tok is None:\n        try:\n            return bytes(tokens).decode(\"utf-8\", errors=\"ignore\")\n        except:\n            return \"\"\n    \n    tok_type, tok_obj = tok\n    if tok_type == \"tiktoken\":\n        return tok_obj.decode(tokens)\n    elif tok_type == \"hf\":\n        return tok_obj.decode(tokens)\n    else:\n        try:\n            return bytes(tokens).decode(\"utf-8\", errors=\"ignore\")\n        except:\n            return \"\"\n\n# -----------------------\n# Generation with temperature and top-k sampling\n# -----------------------\n@torch.no_grad()\ndef generate(model, prompt_tokens: list, max_new_tokens: int = 100, \n             temperature: float = 1.0, top_k: int = 0, device: str = \"cpu\"):\n    \"\"\"Generate text autoregressively from prompt\"\"\"\n    model.eval()\n    \n    # Convert prompt to tensor\n    input_ids = torch.tensor([prompt_tokens], dtype=torch.long).to(device)\n    generated = prompt_tokens.copy()\n    \n    for _ in range(max_new_tokens):\n        # Truncate if exceeds seq_len\n        if input_ids.size(1) > model.seq_len:\n            input_ids = input_ids[:, -model.seq_len:]\n        \n        # Forward pass\n        logits, _ = model(input_ids)\n        \n        # Get logits for last token\n        logits = logits[:, -1, :] / temperature\n        \n        # Apply top-k filtering\n        if top_k > 0:\n            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n            logits[indices_to_remove] = float('-inf')\n        \n        # Sample from distribution\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)\n        \n        # Append to sequence\n        generated.append(next_token.item())\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n        \n        # Optional: stop at special tokens (e.g., 0, 1, 2 for padding/eos)\n        # if next_token.item() in [0, 1, 2]:\n        #     break\n    \n    return generated\n\n# -----------------------\n# Main inference function\n# -----------------------\ndef main():\n    print(\"\\n\" + \"=\"*50)\n    print(\"ðŸ§ª GPT Model Inference Script\")\n    print(\"=\"*50 + \"\\n\")\n    \n    # Load config\n    if os.path.exists(CONFIG_PATH):\n        print(f\"Loading config from: {CONFIG_PATH}\")\n        with open(CONFIG_PATH, \"r\") as f:\n            config = json.load(f)\n        vocab_size = config.get(\"vocab_size\", 256)\n        seq_len = config.get(\"SEQ_LEN\", 256)\n        print(f\"Config loaded: vocab_size={vocab_size}, seq_len={seq_len}\")\n    else:\n        print(\"âš ï¸ Config not found, using defaults\")\n        vocab_size = 256  # byte-level default\n        seq_len = 256\n    \n    # Check if using HF tokenizer\n    tokenizer_type = \"byte_level\"\n    if os.path.exists(META_PATH):\n        with open(META_PATH, \"r\") as f:\n            meta = json.load(f)\n            tokenizer_type = meta.get(\"tokenizer\", \"byte_level\")\n    \n    print(f\"Tokenizer type: {tokenizer_type}\\n\")\n    \n    # Initialize model\n    print(\"Initializing model...\")\n    model = MiniGPT(\n        vocab_size=vocab_size,\n        d_model=512,\n        n_heads=8,\n        n_layers=6,\n        seq_len=seq_len,\n        ffn_expansion=4,\n        dropout=0.1\n    ).to(DEVICE)\n    \n    # Load checkpoint\n    if not os.path.exists(CHECKPOINT_PATH):\n        print(f\"âŒ Checkpoint not found: {CHECKPOINT_PATH}\")\n        return\n    \n    checkpoint = load_checkpoint(CHECKPOINT_PATH, DEVICE)\n    model.load_state_dict(checkpoint[\"model_state\"])\n    model.eval()\n    \n    print(f\"âœ… Model loaded successfully!\")\n    print(f\"Global step: {checkpoint.get('global_step', 'N/A')}\")\n    if 'extra' in checkpoint and 'val_loss' in checkpoint['extra']:\n        print(f\"Val loss: {checkpoint['extra']['val_loss']:.4f}\")\n        print(f\"Perplexity: {checkpoint['extra'].get('perplexity', 'N/A')}\")\n    print()\n    \n    # Interactive loop\n    print(\"=\"*50)\n    print(\"Ready for inference! Type 'quit' to exit.\")\n    print(\"=\"*50 + \"\\n\")\n    \n    while True:\n        prompt = input(\"Enter prompt: \").strip()\n        \n        if prompt.lower() in ['quit', 'exit', 'q']:\n            print(\"Exiting...\")\n            break\n        \n        if not prompt:\n            continue\n        \n        # Encode prompt\n        prompt_tokens = encode_text(prompt, tokenizer_type)\n        print(f\"\\nPrompt tokens ({len(prompt_tokens)}): {prompt_tokens[:20]}...\")\n        \n        # Generate\n        print(f\"Generating {MAX_NEW_TOKENS} tokens...\")\n        generated_tokens = generate(\n            model,\n            prompt_tokens,\n            max_new_tokens=MAX_NEW_TOKENS,\n            temperature=TEMPERATURE,\n            top_k=TOP_K,\n            device=DEVICE\n        )\n        \n        # Decode\n        generated_text = decode_tokens(generated_tokens, tokenizer_type)\n        \n        print(\"\\n\" + \"-\"*50)\n        print(\"Generated text:\")\n        print(\"-\"*50)\n        print(generated_text)\n        print(\"-\"*50 + \"\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T09:05:47.038794Z","iopub.execute_input":"2025-12-02T09:05:47.039134Z","iopub.status.idle":"2025-12-02T09:07:59.603335Z","shell.execute_reply.started":"2025-12-02T09:05:47.039109Z","shell.execute_reply":"2025-12-02T09:07:59.602585Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nðŸ§ª GPT Model Inference Script\n==================================================\n\nLoading config from: /kaggle/input/confic/confic.json\nConfig loaded: vocab_size=50257, seq_len=256\nTokenizer type: tiktoken_gpt2\n\nInitializing model...\nLoading checkpoint from: /kaggle/input/46k-itr/ckpt_step46000.pt\nâœ… Model loaded successfully!\nGlobal step: 46000\n\n==================================================\nReady for inference! Type 'quit' to exit.\n==================================================\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  The capital of India is \n"},{"name":"stdout","text":"âœ… Loaded tiktoken encoder: gpt2\n\nPrompt tokens (5): [464, 3139, 286, 3794, 318]...\nGenerating 100 tokens...\n\n--------------------------------------------------\nGenerated text:\n--------------------------------------------------\nThe capital of India is a popular country, the most popular sport. the rugby is the highest rugby world cup in the british league. the club is the main club (fucnfaa) and the scottish competition in the united states, and has a number of competitions in the united states, including in the united states: the club's premier league, the team is represented in the \"london cup\". the club is a team is a major league which is the british club (ro\n--------------------------------------------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  America has the \n"},{"name":"stdout","text":"\nPrompt tokens (3): [18165, 468, 262]...\nGenerating 100 tokens...\n\n--------------------------------------------------\nGenerated text:\n--------------------------------------------------\nAmerica has the largest producer in the country, especially in the nation. in early april 2007, the company adopted a new \"new american\" that ran all of its own label, which included the last name of the band's \"the band\" on september 16, 2008.\n\non june 7, 2015, a live album, \"the new orleans\" was released at the same time. it was certified $3 million in 2009.\n\non november 7, 2017\n--------------------------------------------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  tata moters \n"},{"name":"stdout","text":"\nPrompt tokens (4): [83, 1045, 2369, 364]...\nGenerating 100 tokens...\n\n--------------------------------------------------\nGenerated text:\n--------------------------------------------------\ntata moters, he left for the first time in the second round of the second time.\n\nhe played the celtics, the youngest evertonio in the first round of the wolverineken cup but was the longest in the summer olympics in the early 1920s. he was a member of the national team that year in the league with their first-round knockout in the league. although the club won the cec cup, he was the scottish cup the second team to\n--------------------------------------------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  JK lakhsmipat university is \n"},{"name":"stdout","text":"\nPrompt tokens (8): [41, 42, 29002, 5796, 541, 265, 6403, 318]...\nGenerating 100 tokens...\n\n--------------------------------------------------\nGenerated text:\n--------------------------------------------------\nJK lakhsmipat university is called \"the \"b.p. r. n. s. t. k. k. h. m. b. s. l. f. l. d. g. in 1976, the school was the official language of the u.s. government (formerly u. u.s. highway 8 at the end of december 1981) to develop the same name (the university of caledonian), established in 1993 at the school building on the site of the district and the\n--------------------------------------------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  exit\n"},{"name":"stdout","text":"Exiting...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# # generate_from_ckpt.py\n# # Usage:\n# #  - Edit CHECKPOINT_PATH and optionally TOKENIZER_NAME\n# #  - Run: python generate_from_ckpt.py\n# # Produces: printed generated text for the prompt below.\n\n# import os\n# import json\n# import math\n# import torch\n# import torch.nn.functional as F\n# from typing import List, Optional\n\n# # --- Edit these ---\n# CHECKPOINT_PATH = \"/kaggle/input/checkpoint/ckpt_step8000.pt\"   # a checkpoint saved by your train script\n# TOKENIZER_NAME = None   # e.g. \"gpt2\" if you used HF tokenizer; None to use byte-level fallback\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# # ------------------\n\n# # If your MiniGPT and component classes are in a module, import them.\n# # If not, paste the MiniGPT and component class definitions here (or run from the same file).\n# # For convenience, we'll import from train_gpt_memmap if it's in the same folder.\n# try:\n#     from train_gpt_memmap import MiniGPT, load_meta, load_memmap_tokens  # type: ignore\n# except Exception:\n#     # If import fails, assume MiniGPT class is in the same file or user will paste it.\n#     MiniGPT = None\n\n# # -----------------------\n# # Tokenizer helpers\n# # -----------------------\n# def load_tokenizer(tokenizer_name: Optional[str]):\n#     if tokenizer_name:\n#         try:\n#             from transformers import AutoTokenizer\n#             tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n#             print(\"Loaded HF tokenizer:\", tokenizer_name)\n#             return tok\n#         except Exception as e:\n#             print(\"Failed to load HF tokenizer:\", e)\n#             print(\"Falling back to byte-level tokenizer.\")\n#     return None\n\n# def encode_text(tok, text: str) -> List[int]:\n#     if tok is not None:\n#         return tok(text, add_special_tokens=False)[\"input_ids\"]\n#     else:\n#         b = text.encode(\"utf-8\", errors=\"ignore\")\n#         return list(b)  # bytes -> ints 0..255\n\n# def decode_tokens(tok, ids: List[int]) -> str:\n#     if tok is not None:\n#         return tok.decode(ids) if hasattr(tok, \"decode\") else tok.convert_tokens_to_string(tok.convert_ids_to_tokens(ids))\n#     else:\n#         # byte-level decode\n#         return bytes([i % 256 for i in ids]).decode(\"utf-8\", errors=\"ignore\")\n\n# # -----------------------\n# # Sampling utilities\n# # -----------------------\n# def top_k_top_p_filtering(logits, top_k=0, top_p=0.0):\n#     \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p). \"\"\"\n#     top_k = int(top_k)\n#     if top_k > 0:\n#         values, _ = torch.topk(logits, top_k)\n#         min_values = values[:, -1].unsqueeze(1)\n#         logits = torch.where(logits < min_values, torch.full_like(logits, -float(\"Inf\")), logits)\n#     if top_p > 0.0:\n#         sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n#         cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n#         # remove tokens with cumulative probability above top_p\n#         sorted_mask = cumulative_probs > top_p\n#         # shift the mask to keep first token above threshold\n#         sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n#         sorted_mask[..., 0] = False\n#         indices_to_remove = sorted_mask.scatter(1, sorted_indices, sorted_mask)\n#         logits = logits.masked_fill(indices_to_remove, -float(\"Inf\"))\n#     return logits\n\n# # -----------------------\n# # Simple generator (no KV caching)\n# # -----------------------\n# def generate_simple(model, tokenizer, prompt: str, max_new_tokens=64, temperature=1.0,\n#                     top_k=50, top_p=0.0, do_sample=True):\n#     model.eval()\n#     tok = load_tokenizer(TOKENIZER_NAME) if tokenizer is None else tokenizer\n#     ids = encode_text(tok, prompt)\n#     input_ids = torch.tensor([ids], dtype=torch.long, device=DEVICE)\n#     generated = ids.copy()\n\n#     for step in range(max_new_tokens):\n#         with torch.no_grad():\n#             logits, _ = model(input_ids)  # (B, T, V)\n#             logits = logits[:, -1, :] / max(1e-8, temperature)  # last token logits\n#             logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n#             probs = F.softmax(logits, dim=-1)\n\n#             if do_sample:\n#                 next_token = torch.multinomial(probs, num_samples=1).squeeze(1).cpu().item()\n#             else:\n#                 next_token = torch.argmax(probs, dim=-1).cpu().item()\n\n#         generated.append(int(next_token))\n#         # feed the newly generated token for next step\n#         input_ids = torch.tensor([generated], dtype=torch.long, device=DEVICE)\n\n#     text = decode_tokens(tok, generated)\n#     return text, generated\n\n# # -----------------------\n# # Faster generator with per-layer KV cache\n# # (Works only if model.blocks return per-layer cache objects â€” we adapt to the MiniGPT used earlier)\n# # -----------------------\n# def generate_with_cache(model, tokenizer, prompt: str, max_new_tokens=64, temperature=1.0,\n#                         top_k=50, top_p=0.0, do_sample=True):\n#     \"\"\"\n#     This implementation keeps per-block cached keys (kh) and reuses them.\n#     It assumes model.blocks is a ModuleList and that each block's attention supports\n#     passing a cache_k argument and returning new_cache_k for that block.\n#     The train script's CausalKVAttention_simple returns per-block kh as `new_cache_k`\n#     but MiniGPT as provided returned only final cache; to use this function you must\n#     adapt MiniGPT.forward to return a list of caches (one per block) or call blocks manually.\n#     Below we call blocks manually to gather caches.\n#     \"\"\"\n#     model.eval()\n#     tok = load_tokenizer(TOKENIZER_NAME) if tokenizer is None else tokenizer\n#     ids = encode_text(tok, prompt)\n#     input_ids = torch.tensor([ids], dtype=torch.long, device=DEVICE)\n#     generated = ids.copy()\n\n#     # initialize per-layer cache to None\n#     layer_caches = [None] * len(model.blocks)  # each entry shape: (B, n_heads, T_cache, head_dim)\n\n#     # first pass: feed the prompt all at once to fill the caches\n#     # We'll run block-by-block so we can collect caches per-block.\n#     with torch.no_grad():\n#         x = model.tok_emb(input_ids) + model.pos_emb[:, : input_ids.size(1), :].to(DEVICE)\n#         new_caches = []\n#         for i, blk in enumerate(model.blocks):\n#             # call block with cache_k (None for prompt)\n#             # We expect block to return (x, new_cache_k)\n#             x, new_cache_k = blk(x, cache_k=layer_caches[i])\n#             new_caches.append(new_cache_k)\n#         layer_caches = new_caches\n#         x = model.ln_f(x)\n#         logits = model.lm_head(x)\n#     # generation loop (one token at a time)\n#     for step in range(max_new_tokens):\n#         # prepare input embedding for last token only\n#         last_token_id = torch.tensor([[generated[-1]]], dtype=torch.long, device=DEVICE)\n#         x = model.tok_emb(last_token_id)  # (1,1,d)\n#         # add positional embedding for next position\n#         pos_idx = len(generated)  # zero-based position to index pos_emb\n#         x = x + model.pos_emb[:, pos_idx : pos_idx + 1, :].to(DEVICE)\n\n#         # pass through blocks reusing caches (append new keys)\n#         new_caches = []\n#         for i, blk in enumerate(model.blocks):\n#             x, new_cache_k = blk(x, cache_k=layer_caches[i])\n#             new_caches.append(new_cache_k)\n#         layer_caches = new_caches\n\n#         x = model.ln_f(x)\n#         logits = model.lm_head(x)  # (1,1,V)\n#         logits = logits[:, -1, :] / max(1e-8, temperature)\n#         logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n#         probs = F.softmax(logits, dim=-1)\n\n#         if do_sample:\n#             next_token = torch.multinomial(probs, num_samples=1).squeeze(1).cpu().item()\n#         else:\n#             next_token = torch.argmax(probs, dim=-1).cpu().item()\n\n#         generated.append(int(next_token))\n\n#     text = decode_tokens(tok, generated)\n#     return text, generated\n\n# # -----------------------\n# # Load model checkpoint\n# # -----------------------\n# def load_model_from_checkpoint(path: str):\n#     ckpt = torch.load(path, map_location=\"cpu\")\n#     # ckpt payload might be {\"model_state\": ..., ...} or a raw state_dict\n#     if \"model_state\" in ckpt:\n#         state = ckpt[\"model_state\"]\n#     else:\n#         state = ckpt\n#     # load model config (if saved) to construct the MiniGPT correctly\n#     # try to infer vocab_size and seq_len from ckpt or config file\n#     config = {}\n#     config_path = os.path.join(os.path.dirname(path), \"config.json\")\n#     if os.path.exists(config_path):\n#         with open(config_path, \"r\", encoding=\"utf-8\") as f:\n#             config = json.load(f)\n#     vocab_size = config.get(\"vocab_size\", 65536)\n#     seq_len = config.get(\"SEQ_LEN\", 256)\n#     # instantiate model (match your training model settings)\n#     if MiniGPT is None:\n#         raise RuntimeError(\"MiniGPT class not available. Import it or copy the class into this file.\")\n#     model = MiniGPT(vocab_size=vocab_size, d_model=512, n_heads=8, n_layers=6, seq_len=seq_len)\n#     model.load_state_dict(state, strict=False)\n#     model.to(DEVICE)\n#     model.eval()\n#     return model\n\n# # -----------------------\n# # Example usage\n# # -----------------------\n# if __name__ == \"__main__\":\n#     # load tokenizer & model\n#     tokenizer = load_tokenizer(TOKENIZER_NAME)\n#     model = load_model_from_checkpoint(CHECKPOINT_PATH)\n\n#     # choose a prompt\n#     prompt = \"In a future where models write code, a developer asked:\"\n#     print(\"Prompt:\", prompt)\n#     print(\"Generating with simple (no-cache) generator...\")\n#     text, ids = generate_simple(model, tokenizer, prompt, max_new_tokens=64, temperature=0.8, top_k=40, top_p=0.9, do_sample=True)\n#     print(\"\\n=== GENERATED (simple) ===\\n\")\n#     print(text)\n#     print(\"\\nToken ids:\", ids[-40:])\n\n#     # if you want faster generation and your model supports per-block caching as above:\n#     try:\n#         print(\"\\nGenerating with cache-enabled generator (faster)...\")\n#         text2, ids2 = generate_with_cache(model, tokenizer, prompt, max_new_tokens=64, temperature=0.8, top_k=40, top_p=0.9, do_sample=True)\n#         print(\"\\n=== GENERATED (with cache) ===\\n\")\n#         print(text2)\n#     except Exception as e:\n#         print(\"Cache generation failed (probably model.forward doesn't return per-block caches). Error:\", e)\n#         print(\"You can still use the simple generator.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T06:07:03.280313Z","iopub.execute_input":"2025-12-02T06:07:03.280589Z","iopub.status.idle":"2025-12-02T06:07:15.140592Z","shell.execute_reply.started":"2025-12-02T06:07:03.280567Z","shell.execute_reply":"2025-12-02T06:07:15.139439Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2032269446.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# load tokenizer & model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOKENIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# choose a prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2032269446.py\u001b[0m in \u001b[0;36mload_model_from_checkpoint\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# instantiate model (match your training model settings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mMiniGPT\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MiniGPT class not available. Import it or copy the class into this file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: MiniGPT class not available. Import it or copy the class into this file."],"ename":"RuntimeError","evalue":"MiniGPT class not available. Import it or copy the class into this file.","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}